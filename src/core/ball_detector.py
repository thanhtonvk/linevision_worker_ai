# =============================================================================
# BALL DETECTOR CLASS - PH√ÅT HI·ªÜN V√Ä TRACKING B√ìNG TENNIS
# =============================================================================

import cv2
import math
import numpy as np
from ultralytics import YOLO
from itertools import combinations
import gc
import torch


def gpu_memory_full(threshold_ratio: float = 0.85):
    """
    Ki·ªÉm tra n·∫øu GPU memory > 85% (ho·∫∑c t√πy ch·ªçn).
    """
    if not torch.cuda.is_available():
        return False
    mem_alloc = torch.cuda.memory_allocated()
    mem_total = torch.cuda.get_device_properties(0).total_memory
    return mem_alloc / mem_total > threshold_ratio


class BallDetector:
    """
    Class ƒë·ªÉ detect v√† track b√≥ng tennis trong video
    Optimized for 12GB GPU with batch inference
    """

    def __init__(
        self,
        model_path="src/models/ball_best.pt",
        person_model_path="src/models/yolov8n.pt",  # Changed to nano
        batch_size=16,  # Optimized for 12GB GPU
    ):
        self.model = YOLO(model_path)
        self.person_model = YOLO(person_model_path)
        self.model_path = model_path
        self.batch_size = batch_size  # Increased for faster processing with 12GB GPU
        self.conf = 0.7

        # Enable half precision for faster inference
        if torch.cuda.is_available():
            self.model.to('cuda')
            self.person_model.to('cuda')

    def read_video(self, video_path):
        """ƒê·ªçc video v√† tr·∫£ v·ªÅ danh s√°ch frames"""
        cap = cv2.VideoCapture(video_path)
        frames = []
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
        finally:
            cap.release()
        return frames

    def reset_model(self):
        """Clear GPU + reload YOLO model."""
        print("‚ö†Ô∏è GPU RAM cao, reload YOLO model...")

        try:
            del self.model
        except:
            pass

        torch.cuda.empty_cache()
        gc.collect()

        self.model = YOLO(self.model_path).to("cuda")
        print("‚úÖ Model loaded l·∫°i th√†nh c√¥ng.")

    def batch_frames(self, frames):
        """Chia frames th√†nh c√°c batch ƒë·ªÉ x·ª≠ l√Ω"""
        return [
            frames[i : i + self.batch_size]
            for i in range(0, len(frames), self.batch_size)
        ]

    def detect_positions(self, frames, show_progress=True):
        """Detect v·ªã tr√≠ b√≥ng trong c√°c frames v·ªõi batch inference

        Args:
            frames: List of video frames
            show_progress: Show progress during inference

        Returns:
            List of (x, y) positions for each frame
        """
        batches = self.batch_frames(frames)
        positions = []
        total_batches = len(batches)

        if show_progress:
            print(f"üéæ Ball detection: {len(frames)} frames, {total_batches} batches (batch_size={self.batch_size})")

        for batch_idx, batch in enumerate(batches):
            if gpu_memory_full():
                self.reset_model()

            # Batch inference with half precision
            results = self.model.predict(
                batch,
                batch=len(batch),  # Use actual batch size
                verbose=False,
                conf=self.conf,
                half=True  # Enable FP16 for faster inference
            )

            for res in results:
                if res.boxes is not None and len(res.boxes) > 0:
                    best_idx = res.boxes.conf.argmax()
                    x, y, w, h = res.boxes.xywh[best_idx].cpu().numpy()
                    positions.append((float(x), float(y)))
                else:
                    positions.append((-1, -1))

            if show_progress and (batch_idx + 1) % 10 == 0:
                progress = (batch_idx + 1) / total_batches * 100
                print(f"   Batch {batch_idx + 1}/{total_batches} ({progress:.1f}%)")

        if show_progress:
            detected = sum(1 for p in positions if p != (-1, -1))
            print(f"‚úÖ Detected {detected}/{len(positions)} ball positions")

        return positions

    def correct_positions(self, positions):
        """S·ª≠a c√°c v·ªã tr√≠ b√≥ng b·∫•t th∆∞·ªùng"""
        corrected_positions = []
        current_group = []

        def process_group(group):
            if len(group) < 2:
                return group
            mean_x = sum(x for x, _ in group) / len(group)
            mean_y = sum(y for _, y in group) / len(group)
            mean_point = (mean_x, mean_y)
            distances = [math.dist(p1, p2) for p1, p2 in combinations(group, 2)]
            avg_dist = sum(distances) / len(distances)
            corrected = []
            for x, y in group:
                d = math.dist((x, y), mean_point)
                if d > avg_dist:
                    dx, dy = x - mean_x, y - mean_y
                    length = math.sqrt(dx * dx + dy * dy)
                    if length > 0:
                        dx, dy = dx / length, dy / length
                        corrected.append(
                            (mean_x + dx * avg_dist, mean_y + dy * avg_dist)
                        )
                    else:
                        corrected.append((mean_x, mean_y))
                else:
                    corrected.append((x, y))
            return corrected

        for i, pos in enumerate(positions):
            if pos == (-1, -1):
                if current_group:
                    corrected_positions.extend(process_group(current_group))
                    current_group = []
                corrected_positions.append(pos)
            else:
                current_group.append(pos)

        if current_group:
            corrected_positions.extend(process_group(current_group))

        return corrected_positions

    def smooth_positions(self, positions, window_size=5):
        """L√†m m∆∞·ª£t v·ªã tr√≠ b√≥ng"""
        if len(positions) < window_size:
            return positions

        smoothed = []
        for i in range(len(positions)):
            if positions[i] == (-1, -1):
                smoothed.append(positions[i])
                continue

            start = max(0, i - window_size // 2)
            end = min(len(positions), i + window_size // 2 + 1)
            window = [pos for pos in positions[start:end] if pos != (-1, -1)]

            if len(window) > 0:
                avg_x = sum(x for x, y in window) / len(window)
                avg_y = sum(y for x, y in window) / len(window)
                smoothed.append((avg_x, avg_y))
            else:
                smoothed.append(positions[i])

        return smoothed

    def interpolate_missing_positions(self, positions, max_gap=10):
        """N·ªôi suy v·ªã tr√≠ b√≥ng b·ªã thi·∫øu"""
        interpolated = []
        i = 0

        while i < len(positions):
            if positions[i] != (-1, -1):
                interpolated.append(positions[i])
                i += 1
            else:
                # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa gap
                start_idx = i
                while i < len(positions) and positions[i] == (-1, -1):
                    i += 1
                end_idx = i
                gap_size = end_idx - start_idx

                if gap_size <= max_gap and start_idx > 0 and end_idx < len(positions):
                    # N·ªôi suy
                    prev_pos = interpolated[-1]
                    next_pos = (
                        positions[end_idx] if end_idx < len(positions) else prev_pos
                    )

                    for j in range(gap_size):
                        ratio = (j + 1) / (gap_size + 1)
                        x = prev_pos[0] + ratio * (next_pos[0] - prev_pos[0])
                        y = prev_pos[1] + ratio * (next_pos[1] - prev_pos[1])
                        interpolated.append((x, y))
                else:
                    # Kh√¥ng n·ªôi suy, gi·ªØ nguy√™n
                    for j in range(gap_size):
                        interpolated.append((-1, -1))

        return interpolated

    def detect_persons(self, frames, person_conf=0.5, show_progress=True):
        """Detect ng∆∞·ªùi trong frames v·ªõi batch inference

        Args:
            frames: List of video frames
            person_conf: Confidence threshold for person detection
            show_progress: Show progress during inference

        Returns:
            List of person detections for each frame
        """
        batches = self.batch_frames(frames)
        person_detections = []
        total_batches = len(batches)

        if show_progress:
            print(f"üë• Person detection: {len(frames)} frames, {total_batches} batches")

        for batch_idx, batch in enumerate(batches):
            if gpu_memory_full():
                torch.cuda.empty_cache()
                gc.collect()

            results = self.person_model.predict(
                batch,
                batch=len(batch),  # Use actual batch size
                verbose=False,
                conf=person_conf,
                half=True  # Enable FP16
            )

            for res in results:
                frame_persons = []
                if res.boxes is not None and len(res.boxes) > 0:
                    for box in res.boxes:
                        if int(box.cls) == 0:  # person class
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            conf = float(box.conf.cpu().numpy()[0])
                            if conf >= person_conf:
                                frame_persons.append(
                                    {
                                        "bbox": (int(x1), int(y1), int(x2), int(y2)),
                                        "conf": conf,
                                    }
                                )
                person_detections.append(frame_persons)

            if show_progress and (batch_idx + 1) % 10 == 0:
                progress = (batch_idx + 1) / total_batches * 100
                print(f"   Batch {batch_idx + 1}/{total_batches} ({progress:.1f}%)")

        if show_progress:
            total_persons = sum(len(p) for p in person_detections)
            print(f"‚úÖ Detected {total_persons} person instances across {len(frames)} frames")

        return person_detections

    def check_ball_person_intersection(
        self, ball_pos, person_bboxes, threshold=80, dynamic_threshold=True
    ):
        """Ki·ªÉm tra b√≥ng c√≥ ch·∫°m ng∆∞·ªùi kh√¥ng"""
        if ball_pos == (-1, -1) or not person_bboxes:
            return False

        ball_x, ball_y = ball_pos

        for person in person_bboxes:
            x1, y1, x2, y2 = person["bbox"]
            conf = person["conf"]

            # Calculate person bbox dimensions
            person_width = x2 - x1
            person_height = y2 - y1
            person_size = max(person_width, person_height)

            # Dynamic threshold based on person size and confidence
            if dynamic_threshold:
                size_factor = min(2.0, max(0.5, person_size / 100))
                conf_factor = min(1.5, max(0.8, conf))
                dynamic_thresh = int(threshold * size_factor * conf_factor)
            else:
                dynamic_thresh = threshold

            # Check if ball is within dynamic threshold distance of person bbox
            if (
                x1 - dynamic_thresh <= ball_x <= x2 + dynamic_thresh
                and y1 - dynamic_thresh <= ball_y <= y2 + dynamic_thresh
            ):
                return True
        return False

    def get_enhanced_direction_change_flags(
        self,
        frames,
        positions,
        angle_threshold=45,
        person_conf=0.5,
        intersection_threshold=80,
    ):
        """Ph√¢n lo·∫°i thay ƒë·ªïi h∆∞·ªõng: b√≥ng ch·∫°m ng∆∞·ªùi vs ch·∫°m ƒë·∫•t"""
        print("ƒêang detect ng∆∞·ªùi trong video...")
        person_detections = self.detect_persons(frames, person_conf=person_conf)

        print("ƒêang ph√¢n t√≠ch thay ƒë·ªïi h∆∞·ªõng...")
        change_points = set(self._detect_direction_changes(positions, angle_threshold))

        direction_flags = []
        person_hit_count = 0
        ground_hit_count = 0

        for i in range(len(positions)):
            if i in change_points:
                ball_pos = positions[i]
                persons = person_detections[i] if i < len(person_detections) else []

                if self.check_ball_person_intersection(
                    ball_pos, persons, threshold=intersection_threshold
                ):
                    direction_flags.append(2)  # B√≥ng ƒë∆∞·ª£c ƒë√°nh b·ªüi ng∆∞·ªùi
                    person_hit_count += 1
                else:
                    direction_flags.append(1)  # B√≥ng ch·∫°m ƒë·∫•t
                    ground_hit_count += 1
            else:
                direction_flags.append(0)  # Kh√¥ng thay ƒë·ªïi h∆∞·ªõng

        print(f"Ph√¢n lo·∫°i ho√†n t·∫•t:")
        print(f"- B√≥ng ch·∫°m ng∆∞·ªùi: {person_hit_count} l·∫ßn")
        print(f"- B√≥ng ch·∫°m ƒë·∫•t: {ground_hit_count} l·∫ßn")
        print(f"- T·ªïng thay ƒë·ªïi h∆∞·ªõng: {person_hit_count + ground_hit_count} l·∫ßn")

        return direction_flags, person_detections

    def _detect_direction_changes(
        self,
        positions,
        angle_threshold=45,
        min_distance=20,
        min_velocity=10,
        min_frames_gap=5,
    ):
        """Ph√°t hi·ªán c√°c ƒëi·ªÉm thay ƒë·ªïi h∆∞·ªõng"""
        change_points = []
        clean_positions = [(i, p) for i, p in enumerate(positions) if p != (-1, -1)]

        if len(clean_positions) < 3:
            return change_points

        for i in range(2, len(clean_positions)):
            idx1, p1 = clean_positions[i - 2]
            idx2, p2 = clean_positions[i - 1]
            idx3, p3 = clean_positions[i]

            v1 = (p2[0] - p1[0], p2[1] - p1[1])
            v2 = (p3[0] - p2[0], p3[1] - p2[1])

            len1 = math.hypot(*v1)
            len2 = math.hypot(*v2)

            if len1 < min_velocity or len2 < min_velocity:
                continue

            dist1 = math.hypot(p2[0] - p1[0], p2[1] - p1[1])
            dist2 = math.hypot(p3[0] - p2[0], p3[1] - p2[1])
            if dist1 < min_distance or dist2 < min_distance:
                continue

            dot = v1[0] * v2[0] + v1[1] * v2[1]
            cos_theta = max(-1, min(1, dot / (len1 * len2)))
            angle = math.degrees(math.acos(cos_theta))

            if angle > angle_threshold:
                if (
                    len(change_points) == 0
                    or idx2 - change_points[-1] >= min_frames_gap
                ):
                    change_points.append(idx2)

        return change_points

    def save_cropped_ball_video(
        self, frames, positions, output_path, crop_size=50, fps=30
    ):
        """L∆∞u video v·ªõi v√πng crop quanh b√≥ng"""
        cropped_frames = []
        prev_crop = None

        for i, frame in enumerate(frames):
            pos = positions[i] if i < len(positions) else (-1, -1)

            if pos == (-1, -1):
                if prev_crop is not None:
                    cropped_frames.append(prev_crop)
                continue

            x, y = int(pos[0]), int(pos[1])
            h, w = frame.shape[:2]
            x1 = max(0, x - crop_size)
            y1 = max(0, y - crop_size)
            x2 = min(w, x + crop_size)
            y2 = min(h, y + crop_size)

            cropped = frame[y1:y2, x1:x2]
            if cropped.size == 0:
                if prev_crop is not None:
                    cropped_frames.append(prev_crop)
                continue

            prev_crop = cropped.copy()
            cropped_frames.append(cropped)

        if not cropped_frames:
            print("Kh√¥ng c√≥ frame n√†o ƒë·ªÉ l∆∞u!")
            return

        height, width = cropped_frames[0].shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        try:
            for frame in cropped_frames:
                out.write(frame)
        finally:
            out.release()
        print(f"ƒê√£ l∆∞u video: {output_path}")
